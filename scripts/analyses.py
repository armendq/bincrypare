# scripts/analyses.py
# -*- coding: utf-8 -*-

import os
import sys
import json
import math
import time
import traceback
from datetime import datetime, timezone
from typing import Dict, Any, List, Tuple

import requests

# ---------- Config (overridable via env) ----------
BINANCE_BASE = os.getenv("BINANCE_BASE", "https://data-api.binance.vision")
# Liquidity / quality gates (permissive first; tune up later)
VOL_USD_MIN_24H = float(os.getenv("VOL_USD_MIN_24H", "500000"))   # min 24h quote volume in USDT
MAX_SPREAD        = float(os.getenv("MAX_SPREAD", "0.02"))        # max relative spread (e.g. 0.02 == 2%)
# Breakout / ATR config
KLINE_INTERVAL = os.getenv("KLINE_INTERVAL", "15m")
KLINE_LIMIT    = int(os.getenv("KLINE_LIMIT", "64"))              # ~16 hours @ 15m (we use last 32 ~ 8h)
LOOKBACK_8H    = int(os.getenv("LOOKBACK_8H", "32"))
ATR_LEN        = int(os.getenv("ATR_LEN", "14"))
BREAKOUT_PAD   = float(os.getenv("BREAKOUT_PAD", "0.001"))        # 0.1% over prior high
# Paths
OUT_DIR        = os.getenv("OUT_DIR", "public_runs/latest")
SUMMARY_PATH   = os.path.join(OUT_DIR, "summary.json")
DEBUG_PATH     = os.path.join(OUT_DIR, "debug_scan.json")
MAPPING_CANDIDATES = [
    "data/revolut_mapping.json",           # generated by mapping step (preferred)
    "data/revolut_mapping.csv",            # legacy
]

# ---------- Helpers ----------
def utcnow_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

def log(msg: str):
    print(f"[analyses] {utcnow_iso()} {msg}", flush=True)

def ensure_dirs():
    os.makedirs(OUT_DIR, exist_ok=True)

def load_mapping() -> List[Dict[str, Any]]:
    """
    Expect a JSON array of objects with keys:
        - 'binance_symbol'  (e.g., 'BTCUSDT')
        - 'revolut_ticker'  (e.g., 'BTC')
    """
    for path in MAPPING_CANDIDATES:
        if not os.path.exists(path):
            continue
        # JSON preferred
        if path.endswith(".json"):
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            # schema: list[ {binance_symbol, revolut_ticker, ...} ]
            if isinstance(data, list) and all(isinstance(x, dict) for x in data):
                if all(("binance_symbol" in x and "revolut_ticker" in x) for x in data):
                    log(f"mapping: loaded {len(data)} rows from {path}")
                    return data
                else:
                    log(f"{path} found but schema not matching; skipping.")
        # CSV fallback
        if path.endswith(".csv"):
            rows = []
            with open(path, "r", encoding="utf-8") as f:
                header = [h.strip() for h in f.readline().strip().split(",")]
                idx_sym = header.index("binance_symbol") if "binance_symbol" in header else -1
                idx_rev = header.index("revolut_ticker") if "revolut_ticker" in header else -1
                if idx_sym < 0 or idx_rev < 0:
                    log(f"{path} found but missing required columns; skipping.")
                else:
                    for line in f:
                        parts = [p.strip() for p in line.strip().split(",")]
                        if len(parts) != len(header):
                            continue
                        rows.append({"binance_symbol": parts[idx_sym], "revolut_ticker": parts[idx_rev]})
            if rows:
                log(f"mapping: loaded {len(rows)} rows from {path}")
                return rows
    raise FileNotFoundError("[analyses] No mapping file found with expected keys "
                            "['binance_symbol', 'revolut_ticker'].")

def get_json(url: str, params: Dict[str, Any], tries: int = 4, timeout: int = 15) -> Any:
    last = None
    for i in range(1, tries + 1):
        try:
            r = requests.get(url, params=params, timeout=timeout)
            if r.status_code == 200:
                return r.json()
            else:
                log(f"GET fail: {url} {r.status_code} (try {i})")
                last = r.text
        except Exception as e:
            log(f"GET exception: {url} ({e}) (try {i})")
        time.sleep(0.8 * i)
    raise RuntimeError(f"GET failed for {url} after {tries} tries; last={last}")

def fetch_24h(symbol: str) -> Dict[str, Any]:
    url = f"{BINANCE_BASE}/api/v3/ticker/24hr"
    return get_json(url, {"symbol": symbol})

def fetch_book(symbol: str) -> Dict[str, Any]:
    url = f"{BINANCE_BASE}/api/v3/ticker/bookTicker"
    return get_json(url, {"symbol": symbol})

def fetch_klines(symbol: str, interval: str, limit: int) -> List[List[Any]]:
    url = f"{BINANCE_BASE}/api/v3/klines"
    return get_json(url, {"symbol": symbol, "interval": interval, "limit": limit})

def to_float(x: Any, default=0.0) -> float:
    try:
        return float(x)
    except Exception:
        return default

def true_range(prev_close, high, low):
    return max(high - low, abs(high - prev_close), abs(low - prev_close))

def compute_atr(closes: List[float], highs: List[float], lows: List[float], period: int) -> float:
    if len(closes) < period + 1:
        return float("nan")
    trs = []
    for i in range(1, len(closes)):
        trs.append(true_range(closes[i-1], highs[i], lows[i]))
    if len(trs) < period:
        return float("nan")
    return sum(trs[-period:]) / period

# ---------- Core scanning ----------
def scan_symbols(mapping_rows: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    universe = []
    debug = {
        "rejected": [],
        "accepted": [],
        "meta": {
            "VOL_USD_MIN_24H": VOL_USD_MIN_24H,
            "MAX_SPREAD": MAX_SPREAD,
            "interval": KLINE_INTERVAL,
            "kline_limit": KLINE_LIMIT,
            "lookback_8h": LOOKBACK_8H,
            "atr_len": ATR_LEN,
            "breakout_pad": BREAKOUT_PAD,
            "binance_base": BINANCE_BASE,
        },
    }

    # 1) Liquidity & spread screening
    for row in mapping_rows:
        symbol = row["binance_symbol"]
        rev = row["revolut_ticker"]
        if not symbol.endswith("USDT"):
            debug["rejected"].append({"symbol": symbol, "revolut": rev, "reason": "non-USDT"})
            continue

        try:
            t24 = fetch_24h(symbol)
            book = fetch_book(symbol)
        except Exception as e:
            debug["rejected"].append({"symbol": symbol, "revolut": rev, "reason": f"fetch_fail:{e}"})
            continue

        vol_usd = to_float(t24.get("quoteVolume"))
        bid = to_float(book.get("bidPrice"))
        ask = to_float(book.get("askPrice"))
        mid = (bid + ask) / 2.0 if bid > 0 and ask > 0 else 0.0
        spread = (ask - bid) / mid if mid > 0 else 1.0

        if vol_usd < VOL_USD_MIN_24H:
            debug["rejected"].append({"symbol": symbol, "revolut": rev, "reason": "vol", "vol_usd": vol_usd})
            continue
        if spread > MAX_SPREAD:
            debug["rejected"].append({"symbol": symbol, "revolut": rev, "reason": "spread", "spread": spread})
            continue

        universe.append({
            "symbol": symbol,
            "revolut": rev,
            "vol_usd": vol_usd,
            "spread": spread,
            "mid": mid,
        })

    if not universe:
        log("FATAL: Universe empty after liquidity/spread filters")
        # Still emit a summary & debug so the workflow doesn't break your pages
        return [], debug

    log(f"universe size after filters: {len(universe)}")

    # 2) Detect ~8h breakouts on 15m candles, compute ATR/levels
    candidates: List[Dict[str, Any]] = []
    for u in universe:
        sym = u["symbol"]
        try:
            kl = fetch_klines(sym, KLINE_INTERVAL, KLINE_LIMIT)
        except Exception as e:
            debug["rejected"].append({"symbol": sym, "revolut": u["revolut"], "reason": f"kline_fail:{e}"})
            continue

        # Parse OHLCV
        closes, highs, lows = [], [], []
        for k in kl:
            # [openTime, open, high, low, close, volume, closeTime, ...]
            highs.append(to_float(k[2]))
            lows.append(to_float(k[3]))
            closes.append(to_float(k[4]))

        if len(closes) < LOOKBACK_8H + 1:
            debug["rejected"].append({"symbol": sym, "revolut": u["revolut"], "reason": "insufficient_candles"})
            continue

        last_close = closes[-1]
        last_high = highs[-1]
        last_low = lows[-1]

        prior_window_high = max(highs[-(LOOKBACK_8H+1):-1])  # max of prior ~8h (exclude last bar)
        breakout = last_close > prior_window_high * (1.0 + BREAKOUT_PAD)

        atr = compute_atr(closes, highs, lows, ATR_LEN)
        if not math.isfinite(atr) or atr <= 0:
            debug["rejected"].append({"symbol": sym, "revolut": u["revolut"], "reason": "atr_nan"})
            continue

        if breakout:
            # Use *breakout bar low* as stop
            stop = last_low
            entry = last_close
            t1 = entry + 0.8 * atr
            t2 = entry + 1.5 * atr
            rr = (entry - stop) / atr if atr > 0 else float("nan")
            c = {
                "ticker": u["revolut"],
                "binance_symbol": sym,
                "entry": round(entry, 10),
                "stop": round(stop, 10),
                "atr": round(atr, 10),
                "t1": round(t1, 10),
                "t2": round(t2, 10),
                "vol_usd_24h": u["vol_usd"],
                "spread": u["spread"],
                "r_over_atr": rr,
            }
            candidates.append(c)
            debug["accepted"].append({"symbol": sym, "revolut": u["revolut"], "reason": "breakout"})

    return candidates, debug

# ---------- Output writers ----------
def write_json(path: str, data: Any):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, sort_keys=False)

def summary_payload(cands: List[Dict[str, Any]]) -> Dict[str, Any]:
    # Keep the envelope your consumer expects
    payload = {
        "generated_at": utcnow_iso(),
        "regime": {"ok": True},                      # simple placeholder; wire up to your model if needed
        "equity": None,                               # can be filled by upstream if you track it
        "cash": None,
        "signals": {
            "type": "C" if cands else "H",
            "text": "Candidates found" if cands else "Hold",
        },
        "candidates": cands,
    }
    return payload

# ---------- Main ----------
def main() -> int:
    ensure_dirs()
    try:
        mapping_rows = load_mapping()
    except Exception as e:
        log(f"FATAL: {e}")
        # Emit a minimal summary so downstream fetchers don't fail
        write_json(SUMMARY_PATH, summary_payload([]))
        return 0

    try:
        cands, debug = scan_symbols(mapping_rows)
    except Exception as e:
        log(f"scan exception: {e}\n{traceback.format_exc()}")
        cands, debug = [], {"error": str(e)}

    # Sort candidates by 24h quote volume desc, then r/ATR desc
    if cands:
        cands.sort(key=lambda x: (x.get("vol_usd_24h", 0.0), x.get("r_over_atr", 0.0)), reverse=True)

    # Write outputs
    write_json(SUMMARY_PATH, summary_payload(cands))
    write_json(DEBUG_PATH, debug)

    log(f"wrote summary with {len(cands)} candidates -> {SUMMARY_PATH}")
    return 0

if __name__ == "__main__":
    sys.exit(main())