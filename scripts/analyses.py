#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Analyses runner
- Loads Revolutâ†”Binance mapping from data/revolut_mapping.json
  (expects items with keys: "revolut_ticker", "binance_symbol")
- Fetches Binance klines with robust retries (headers to avoid 451)
- Detects ~8h momentum bursts using 1h bars (8 bars)
- Produces public_runs/latest/summary.json (signals.type="C" if candidates exist, else "H")
- Also writes public_runs/latest/debug.json for inspection
"""

import os
import sys
import json
import time
import math
import pathlib
import statistics
from typing import Dict, Any, List, Tuple
import requests

# ---------------------------
# Paths
# ---------------------------

ROOT = pathlib.Path(__file__).resolve().parents[1]
DATA_DIR = ROOT / "data"
PUB_DIR = ROOT / "public_runs" / "latest"
PUB_DIR.mkdir(parents=True, exist_ok=True)

SUMMARY_PATH = PUB_DIR / "summary.json"
DEBUG_PATH = PUB_DIR / "debug.json"
LOG_PATH = PUB_DIR / "run.log"

MAPPING_FILE = DATA_DIR / "revolut_mapping.json"  # generated by mapping step

# ---------------------------
# Config
# ---------------------------

BINANCE_BASE = "https://api.binance.com"
KLINES_EP = "/api/v3/klines"        # requires symbol, interval, limit
TICKER_24H_EP = "/api/v3/ticker/24hr"  # optional safety

# request headers to reduce 451 / bot blocks
REQ_HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; analyses-bot/1.0; +https://github.com/armendq/revolut_crypto_mapping)",
    "Accept": "application/json,text/plain,*/*",
    "Connection": "keep-alive",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
    "Origin": "https://api.binance.com",
    "Referer": "https://api.binance.com/",
}
TIMEOUT = 20
RETRIES = 4
RETRY_BACKOFF = 1.5   # exponential

# Scan settings
INTERVAL = "1h"   # 1-hour bars to approximate 8h windows
BARS = 120        # enough for ATR calc and context
WINDOW_HOURS = 8  # "8h spike"
MIN_PCT_RISE = 0.20   # >= +20% in last 8 hours (tuneable)
VOL_MULT = 1.8        # last-8h vs prior-8h volume multiple
ATR_LEN = 14

# Rate limiting
SLEEP_BETWEEN = 0.06  # seconds between symbol requests (~16/s max burst)
CHUNK_PAUSE = 1.0     # pause between chunks

# ---------------------------
# Helpers
# ---------------------------

DEBUG_LOG: Dict[str, Any] = {"events": []}

def log(event: str, **fields):
    rec = {"t": time.strftime("%Y-%m-%d %H:%M:%S UTC", time.gmtime()), "event": event}
    rec.update(fields)
    DEBUG_LOG["events"].append(rec)
    # lightweight line log
    with open(LOG_PATH, "a", encoding="utf-8") as lf:
        lf.write(f"[analyses] {rec['t']} {event} {fields}\n")


def write_json(path: pathlib.Path, payload: Any):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)


def load_mapping() -> List[Dict[str, str]]:
    """
    Returns list of dicts: {revolut_ticker, binance_symbol}
    The generator step creates data/revolut_mapping.json in that schema.
    We also tolerate:
      - a dict of {revolut_ticker: binance_symbol}
      - a list of pairs
    """
    if not MAPPING_FILE.exists():
        raise FileNotFoundError(
            "[analyses] No mapping file found with expected keys "
            "['binance_symbol','revolut_ticker']. Add mapping or adjust DEFAULT_MAPPING_FILES."
        )

    with open(MAPPING_FILE, "r", encoding="utf-8") as f:
        raw = json.load(f)

    out: List[Dict[str, str]] = []

    if isinstance(raw, list):
        # Either list of dicts or list of [rev, binance]
        for item in raw:
            if isinstance(item, dict) and "revolut_ticker" in item and "binance_symbol" in item:
                out.append({"revolut_ticker": item["revolut_ticker"], "binance_symbol": item["binance_symbol"]})
            elif isinstance(item, list) and len(item) == 2:
                out.append({"revolut_ticker": item[0], "binance_symbol": item[1]})
    elif isinstance(raw, dict):
        # dict mapping rev->binance
        for rev, sym in raw.items():
            out.append({"revolut_ticker": rev, "binance_symbol": sym})
    else:
        raise ValueError("Unsupported mapping JSON schema.")

    # dedupe and keep only USDT pairs on Binance where provided like BTCUSDT
    dedup: Dict[str, Dict[str, str]] = {}
    for row in out:
        rev = row["revolut_ticker"].strip().upper()
        sym = row["binance_symbol"].strip().upper()
        if rev and sym:
            dedup[rev] = {"revolut_ticker": rev, "binance_symbol": sym}

    final = list(dedup.values())
    log("mapping_loaded", count=len(final))
    return final


def http_get_json(url: str, params: Dict[str, Any]) -> Any:
    last_err = None
    for i in range(1, RETRIES + 1):
        try:
            r = requests.get(url, params=params, headers=REQ_HEADERS, timeout=TIMEOUT)
            if r.status_code == 200:
                return r.json()
            # retry 4xx like 451 with backoff
            last_err = f"http {r.status_code}"
            log("http_non_200", url=url, status=r.status_code, try_num=i)
        except Exception as e:
            last_err = str(e)
            log("http_exc", url=url, err=last_err, try_num=i)
        # backoff
        time.sleep((RETRY_BACKOFF ** (i - 1)))
    raise RuntimeError(f"GET failed for {url} : {last_err}")


def fetch_klines(symbol: str, interval: str = INTERVAL, limit: int = BARS) -> List[List[Any]]:
    """
    Returns klines arrays: [openTime, open, high, low, close, volume, closeTime, ...]
    """
    url = BINANCE_BASE + KLINES_EP
    params = {"symbol": symbol, "interval": interval, "limit": limit}
    data = http_get_json(url, params)
    if not isinstance(data, list) or not data:
        raise RuntimeError("Empty klines")
    return data


def to_float(x: Any) -> float:
    try:
        return float(x)
    except Exception:
        return math.nan


def calc_atr(kl: List[List[Any]], length: int = ATR_LEN) -> float:
    trs: List[float] = []
    for i in range(1, len(kl)):
        prev_close = to_float(kl[i-1][4])
        high = to_float(kl[i][2])
        low = to_float(kl[i][3])
        tr = max(high - low, abs(high - prev_close), abs(low - prev_close))
        trs.append(tr)
    if len(trs) < length:
        return float("nan")
    return statistics.fmean(trs[-length:])


def detect_spike(kl: List[List[Any]]) -> Dict[str, Any]:
    """
    Looks at last 8 bars (1h each): price change & volume surge.
    Returns {} if no signal; else dict with 'entry','stop','atr','t1','t2'
    """
    if len(kl) < (ATR_LEN + WINDOW_HOURS + 5):
        return {}

    close_series = [to_float(row[4]) for row in kl]
    low_series = [to_float(row[3]) for row in kl]
    vol_series = [to_float(row[5]) for row in kl]

    last_close = close_series[-1]
    ref_close = close_series[-(WINDOW_HOURS + 1)]
    if ref_close <= 0 or not math.isfinite(ref_close):
        return {}

    pct = (last_close - ref_close) / ref_close

    # volume windows
    last8_vol = sum(vol_series[-WINDOW_HOURS:])
    prior8_vol = sum(vol_series[-(2*WINDOW_HOURS):-WINDOW_HOURS]) + 1e-12

    vol_ok = (last8_vol / prior8_vol) >= VOL_MULT
    pct_ok = pct >= MIN_PCT_RISE

    if not (pct_ok and vol_ok):
        return {}

    # breakout bar = the hour where we exceeded prior 8h high
    last8_high = max([to_float(row[2]) for row in kl[-WINDOW_HOURS:]])
    # find index of first bar in last 8 that closed above previous range high
    breakout_low = min(low_series[-WINDOW_HOURS:])  # conservative if unknown; could refine

    atr = calc_atr(kl, ATR_LEN)
    if not math.isfinite(atr) or atr <= 0:
        return {}

    entry = last_close
    stop = breakout_low
    t1 = entry + 0.8 * atr
    t2 = entry + 1.5 * atr

    return {
        "entry": round(entry, 8),
        "stop": round(stop, 8),
        "atr": round(atr, 8),
        "t1": round(t1, 8),
        "t2": round(t2, 8),
        "pct8h": round(pct * 100.0, 2),
        "vol_mult": round((last8_vol / prior8_vol), 2),
    }


# ---------------------------
# Main
# ---------------------------

def main() -> int:
    start = time.time()
    write_json(DEBUG_PATH, {"events": []})  # create/clear

    try:
        mapping = load_mapping()
    except Exception as e:
        log("fatal_mapping", err=str(e))
        write_json(SUMMARY_PATH, {
            "status": "error",
            "generated_at_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            "error": f"mapping: {e}",
            "signals": {"type": "H", "text": "Hold and wait."},
            "candidates": []
        })
        write_json(DEBUG_PATH, DEBUG_LOG)
        return 1

    symbols: List[Tuple[str, str]] = []
    for row in mapping:
        rev = row["revolut_ticker"].upper().strip()
        sym = row["binance_symbol"].upper().strip()
        # keep only standard spot symbols (e.g., BTCUSDT, not leveraged tokens)
        if not sym or "UPUSDT" in sym or "DOWNUSDT" in sym:
            continue
        symbols.append((rev, sym))

    total = len(symbols)
    log("universe", count=total)

    candidates: List[Dict[str, Any]] = []
    scanned = 0

    for idx, (rev, sym) in enumerate(symbols, 1):
        try:
            # Fetch & evaluate
            kl = fetch_klines(sym, interval=INTERVAL, limit=BARS)
            sig = detect_spike(kl)
            if sig:
                candidates.append({
                    "revolut_ticker": rev,
                    "binance_symbol": sym,
                    **sig
                })
        except Exception as e:
            log("symbol_fail", sym=sym, err=str(e))
        finally:
            scanned += 1
            # Lightweight progress
            if scanned % 25 == 0 or scanned == total:
                log("progress", scanned=scanned, total=total)
            time.sleep(SLEEP_BETWEEN)

        # soft chunk pacing
        if idx % 120 == 0:
            time.sleep(CHUNK_PAUSE)

    # Sort candidates by strength (pct8h then vol_mult)
    candidates.sort(key=lambda x: (x.get("pct8h", 0.0), x.get("vol_mult", 0.0)), reverse=True)

    # Build summary
    summary: Dict[str, Any] = {
        "status": "ok",
        "generated_at_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "universe": total,
        "scanned": scanned,
        "params": {
            "interval": INTERVAL,
            "window_hours": WINDOW_HOURS,
            "min_pct_rise": MIN_PCT_RISE,
            "vol_mult": VOL_MULT,
            "atr_len": ATR_LEN
        },
        "regime": {"ok": True, "note": "placeholder"},  # fill in if you have regime logic
        "signals": {},
        "candidates": candidates
    }

    if candidates:
        summary["signals"] = {"type": "C", "text": "Candidates available."}
    else:
        summary["signals"] = {"type": "H", "text": "Hold and wait."}

    write_json(SUMMARY_PATH, summary)
    write_json(DEBUG_PATH, DEBUG_LOG)
    log("done", took_s=round(time.time() - start, 2), candidates=len(candidates))
    return 0


if __name__ == "__main__":
    sys.exit(main())